"""Simplified LLM Client - –æ–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ llama-cpp-python"""

import logging
import json
import re
from typing import Optional, Dict, Any

logger = logging.getLogger(__name__)


class LLMClient:
    """–£–ø—Ä–æ—â—ë–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM"""
    
    # Thread-safety: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö GPU –æ–ø–µ—Ä–∞—Ü–∏–π
    from threading import Semaphore
    _inference_semaphore = Semaphore(2)  # Max 2 concurrent requests
    
    def __init__(self, model_path: str, n_ctx: int = 2048, n_gpu_layers: int = -1):
        """
        Args:
            model_path: –ü—É—Ç—å –∫ GGUF –º–æ–¥–µ–ª–∏
            n_ctx: –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            n_gpu_layers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ –Ω–∞ GPU (-1 = –≤—Å–µ)
        """
        self.model_path = model_path
        self.llm = None
        self.n_ctx = n_ctx
        self.n_gpu_layers = n_gpu_layers
        self._inference_count = 0
        self._init_model(n_ctx, n_gpu_layers)
    
    def _init_model(self, n_ctx: int, n_gpu_layers: int):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        try:
            from llama_cpp import Llama
            
            logger.info(f"Loading model: {self.model_path}")
            logger.info(f"GPU layers: {n_gpu_layers}, context: {n_ctx}")
            
            self.llm = Llama(
                model_path=self.model_path,
                n_ctx=n_ctx,
                n_gpu_layers=n_gpu_layers,
                n_batch=512,
                n_threads=1,
                use_mlock=True,
                verbose=False
            )
            
            logger.info("‚úÖ Model loaded successfully")
            logger.info(f"VRAM limit: 2 concurrent requests, context={n_ctx}")
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            self.llm = None
    
    def generate(self, prompt: str, max_tokens: int = 512, 
                 temperature: float = 0.1, stop: Optional[list] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å concurrency control
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç
            max_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (0.0-1.0)
            stop: –°—Ç–æ–ø-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if not self.llm:
            logger.error("Model not initialized")
            return ""
        
        # Acquire semaphore –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è concurrent GPU operations
        with self._inference_semaphore:
            self._inference_count += 1
            
            try:
                result = self.llm(
                    prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    stop=stop or ["</s>", "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:", "User:"],
                    echo=False
                )
                
                text = result.get('choices', [{}])[0].get('text', '').strip()
                
                # üöÄ GPU cleanup –ø–æ—Å–ª–µ inference –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è OOM
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                        logger.debug("[GPU] Cache cleared after inference")
                except ImportError:
                    pass  # torch not available
                except Exception as e:
                    logger.warning(f"GPU cleanup failed: {e}")
                
                # Warn –µ—Å–ª–∏ –º–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–æ–≤ (–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π memory leak)
                if self._inference_count % 50 == 0:
                    logger.info(f"[LLM] Completed {self._inference_count} inferences")
                
                return text
            except Exception as e:
                logger.error(f"Generation failed: {e}")
                return ""
    
    def close(self):
        """Explicitly close llama.cpp context to prevent memory leak"""
        if self.llm:
            try:
                # llama-cpp-python context cleanup
                if hasattr(self.llm, 'close'):
                    self.llm.close()
                self.llm = None
                logger.info("‚úÖ LLM context closed (preventing memory leak)")
            except Exception as e:
                logger.warning(f"Error closing LLM context: {e}")
    
    def __del__(self):
        """Ensure cleanup on garbage collection"""
        self.close()
    
    def generate_json(self, prompt: str, schema: Dict[str, str], 
                     max_tokens: int = 512) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è JSON —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π —Å—Ö–µ–º—ã
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π –≤–µ—Ä–Ω—É—Ç—å JSON
            schema: –û–ø–∏—Å–∞–Ω–∏–µ –ø–æ–ª–µ–π {field: description}
            max_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –ø—Ä–æ–º–ø—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ JSON —Ñ–æ—Ä–º–∞—Ç–∞
        full_prompt = f"""{prompt}

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –°–¢–†–û–ì–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤:
{{
{chr(10).join(f'  "{field}": "..."' for field in schema.keys())}
}}
"""
        
        text = self.generate(full_prompt, max_tokens=max_tokens, temperature=0.1)
        
        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
        try:
            # –ò—â–µ–º JSON –±–ª–æ–∫
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                result = json.loads(json_str)
                logger.debug(f"Parsed JSON successfully")
                return result
        except json.JSONDecodeError as e:
            logger.warning(f"JSON parse failed: {e}")
        
        # Fallback: –∏–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª—è regex
        result = {}
        for field in schema.keys():
            pattern = rf'"{field}"\s*:\s*"([^"]*)"'
            match = re.search(pattern, text)
            if match:
                result[field] = match.group(1)
            else:
                result[field] = ""
        
        logger.debug(f"Extracted fields via regex: {list(result.keys())}")
        return result
