import logging
import json
import re
import os
import tempfile
from typing import Dict, Optional, List, Tuple
from docx import Document
from pathlib import Path

# LangChain –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
try:
    from langchain.schema import Document as LangChainDocument
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False

# RAG –º–æ–¥—É–ª–∏
try:
    from agents.docling_parser import DoclingParser
    from agents.rag_chunking import RAGChunkingPipeline
    from agents.pre_retrieval import PreRetrievalPipeline
    from agents.post_retrieval import PostRetrievalPipeline
    from agents.vector_store import RAGVectorStore
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫ –¥–ª—è —Å—Ç–∞—Ä—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
try:
    import textract
except ImportError:
    textract = None

logger = logging.getLogger(__name__)

class DocumentProcessor:
    """
    –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM –∏ Advanced RAG
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - Direct LLM –∞–Ω–∞–ª–∏–∑ (–±–∞–∑–æ–≤—ã–π —Ä–µ–∂–∏–º)
    - Advanced RAG (–ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ)
    """
    
    def __init__(self, model_path: Optional[str] = None, enable_rag: bool = False):
        self.llm_client = None
        self.model_path = model_path
        self.rag_enabled = False
        
        # RAG –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.docling_parser = None
        self.chunking_pipeline = None
        self.pre_retrieval = None
        self.post_retrieval = None
        self.vector_store = None
        
        if model_path:
            self._init_local_model(model_path)
        
        if enable_rag and RAG_AVAILABLE:
            self._init_rag_components()

    def _init_local_model(self, model_path: str):
        try:
            from llama_cpp import Llama
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {model_path}")
            
            self.llm_client = Llama(
                model_path=model_path,
                n_ctx=16384,
                n_gpu_layers=35,
                verbose=False
            )
            logger.info(f"[OK] –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (Context: 16k)")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            self.llm_client = None
    
    def _init_rag_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        try:
            if not LANGCHAIN_AVAILABLE or not RAG_AVAILABLE:
                logger.warning("LangChain –∏–ª–∏ RAG –º–æ–¥—É–ª–∏ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã")
                return
            
            from sentence_transformers import SentenceTransformer
            
            logger.info("[RAG] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Advanced RAG –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
            
            # Docling –ø–∞—Ä—Å–µ—Ä
            self.docling_parser = DoclingParser()
            logger.info("  ‚úì Docling Parser")
            
            # Chunking
            self.chunking_pipeline = RAGChunkingPipeline(
                chunk_size=1024,
                chunk_overlap=256,
                strategy="semantic"
            )
            logger.info("  ‚úì RAG Chunking Pipeline")
            
            # Pre-Retrieval
            self.pre_retrieval = PreRetrievalPipeline(llm_client=self.llm_client)
            logger.info("  ‚úì Pre-Retrieval Pipeline")
            
            # Post-Retrieval
            self.post_retrieval = PostRetrievalPipeline(use_reranking=True)
            logger.info("  ‚úì Post-Retrieval Pipeline")
            
            # Vector Store
            embeddings = SentenceTransformer('intfloat/multilingual-e5-base')
            self.vector_store = RAGVectorStore(
                embeddings=embeddings,
                store_type="faiss",
                store_path="data/vector_store"
            )
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å
            if not self.vector_store.load():
                logger.info("  ‚úì Vector Store (–Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å)")
            else:
                logger.info("  ‚úì Vector Store (–∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è)")
            
            self.rag_enabled = True
            logger.info("‚úÖ Advanced RAG —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG: {e}")
            self.rag_enabled = False
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –≤–ª–æ–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ .doc, .docx, .pdf)"""
        texts = []

        for att in attachments or []:
            filename = att.get("filename", "")
            file_data = att.get("data")
            
            if not filename or not file_data:
                continue

            suffix = Path(filename).suffix.lower()
            if suffix not in ['.doc', '.docx', '.pdf']:
                continue

            try:
                # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
                    tmp_file.write(file_data)
                    tmp_path = tmp_file.name
                
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {filename}")
                extracted_text = ""

                # .DOCX
                if suffix == ".docx":
                    try:
                        doc = Document(tmp_path)
                        extracted_text = "\n".join(p.text for p in doc.paragraphs if p.text.strip())
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è .docx {filename}: {e}")

                # .DOC
                elif suffix == ".doc":
                    if textract:
                        try:
                            extracted_text = textract.process(tmp_path).decode('utf-8')
                        except: pass
                    
                    if not extracted_text:
                        # –ë–∏–Ω–∞—Ä–Ω—ã–π —Ñ–æ–ª–ª–±—ç–∫
                        with open(tmp_path, 'rb') as f:
                            binary = f.read()
                            # –§–∏–ª—å—Ç—Ä: –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É (C0-FF) –∏ ASCII
                            text = ''.join(chr(b) if (0xC0 <= b <= 0xFF) or (32 <= b < 127) or b in [10, 13] else ' ' for b in binary)
                            extracted_text = re.sub(r'\s+', ' ', text).strip()

                # .PDF
                elif suffix == ".pdf":
                    try:
                        import PyPDF2
                        with open(tmp_path, 'rb') as pdf_file:
                            reader = PyPDF2.PdfReader(pdf_file)
                            extracted_text = ""
                            for page_num in range(len(reader.pages)):
                                page = reader.pages[page_num]
                                extracted_text += page.extract_text() + "\n"
                            extracted_text = extracted_text.strip()
                    except Exception as e:
                        logger.debug(f"PyPDF2 –Ω–µ –ø–æ–º–æ–≥ —Å {filename}: {e}")
                        try:
                            import pdfplumber
                            with pdfplumber.open(tmp_path) as pdf:
                                extracted_text = ""
                                for page in pdf.pages:
                                    extracted_text += page.extract_text() or ""
                                    extracted_text += "\n"
                            extracted_text = extracted_text.strip()
                        except Exception as e2:
                            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF {filename}: {e2}")

                if extracted_text and len(extracted_text) > 50:
                    texts.append(f"--- –î–û–ö–£–ú–ï–ù–¢: {filename} ---\n{extracted_text}")
                else:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ {filename}")

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ {filename}: {e}")
            finally:
                if 'tmp_path' in locals() and os.path.exists(tmp_path):
                    try: os.unlink(tmp_path)
                    except: pass

        return "\n\n".join(texts)

    def extract_info(self, email_text: str, email_subject: str, attachments: list = None) -> Dict:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è LLM"""
        
        contract_text = self.extract_text_from_attachments(attachments)

        if contract_text.strip():
            logger.info("üìÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è LLM...")
            
            # –ò–ó–ú–ï–ù–ï–ù–ò–ï 2: –£–º–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ (Head + Tail)
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 7000 —Å–∏–º–≤–æ–ª–æ–≤ (–ü—Ä–µ–∞–º–±—É–ª–∞, –ü—Ä–µ–¥–º–µ—Ç) –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3000 (–†–µ–∫–≤–∏–∑–∏—Ç—ã, –ü–æ–¥–ø–∏—Å–∏)
            if len(contract_text) > 12000:
                full_text = contract_text[:8000] + "\n\n...[–ü–†–û–ü–£–°–ö –°–¢–ê–ù–î–ê–†–¢–ù–´–• –£–°–õ–û–í–ò–ô]...\n\n" + contract_text[-4000:]
                logger.info(f"‚úÇÔ∏è –¢–µ–∫—Å—Ç —Å–æ–∫—Ä–∞—â–µ–Ω: {len(contract_text)} -> {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            else:
                full_text = contract_text
        else:
            logger.warning("[WARN] –¢–µ–∫—Å—Ç –≤–ª–æ–∂–µ–Ω–∏–π –ø—É—Å—Ç! –ò—Å–ø–æ–ª—å–∑—É—é —Ç–µ–ª–æ –ø–∏—Å—å–º–∞.")
            # –ï—Å–ª–∏ —Ç–µ–ª–æ –ø–∏—Å—å–º–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω–æ–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –º—ã –ø–æ–¥—Å—Ç–∞–≤–∏–ª–∏ —à–∞–±–ª–æ–Ω),
            # –ø–µ—Ä–µ–¥–∞—ë–º –≤ LLM —Ç–æ–ª—å–∫–æ –µ–≥–æ ‚Äî —á—Ç–æ–±—ã —Ç–µ–º–∞/–∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø–∏—Å—å–º–∞ –Ω–µ –∑–∞–¥–∞–≤–∞–ª–∏ –æ—Ç–≤–µ—Ç.
            if email_text and len(email_text) > 500:
                full_text = email_text
            else:
                full_text = f"–¢–ï–ú–ê: {email_subject}\n–¢–ï–ö–°–¢ –ü–ò–°–¨–ú–ê:\n{email_text}"

        if self.llm_client:
            return self._extract_with_llm(full_text, email_subject)
        else:
            return self._extract_simple(full_text, email_subject)

    def _extract_with_llm(self, text_input: str, email_subject: str) -> Dict:
        """–ó–∞–ø—Ä–æ—Å –∫ LLM —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º –≤—ã–≤–æ–¥–∞"""
        # –£–ª—É—á—à—ë–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 585+ —Ä–µ–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ –∫–∞–∫ –æ–±—É—á–∞—é—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã
        prompt = f"""system
–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏—Å—Ç —Å –æ–ø—ã—Ç–æ–º —Ä–∞–±–æ—Ç—ã —Å 585+ —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∏–º–∏ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞–º–∏.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–æ–≥–æ–≤–æ—Ä—ã –∏ –∏–∑–≤–ª–µ–∫–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ –¥–µ—Ç–∞–ª–∏.

–ò–ù–°–¢–†–£–ö–¶–ò–ò:
1. –ü—Ä–æ—á–∏—Ç–∞–π –¢–û–õ–¨–ö–û —Ç–µ–∫—Å—Ç –¥–æ–≥–æ–≤–æ—Ä–∞ (–±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∞–π–ª–æ–≤, –±–µ–∑ —Ç–µ–º—ã –ø–∏—Å—å–º–∞)
2. –ù–∞–π–¥–∏ —Å—Ç–æ—Ä–æ–Ω—ã –¥–æ–≥–æ–≤–æ—Ä–∞ (–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏, –ò–ü, –§–ò–û)
3. –û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–î–æ–≥–æ–≤–æ—Ä/–ê–∫—Ç/–î–æ–ø.—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ/–¥—Ä.)
4. –î–∞–π —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç–æ–µ —Ä–µ–∑—é–º–µ 3-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (300-600 —Å–∏–º–≤–æ–ª–æ–≤) —Å –∫–ª—é—á–µ–≤—ã–º–∏ –¥–µ—Ç–∞–ª—è–º–∏
5. –ù–∞–π–¥–∏ —Å—É–º–º—É –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ (—á–∏—Å–ª–∞ + –≤–∞–ª—é—Ç–∞), —Å—Ä–æ–∫–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è, –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ –ª–∏—Ü–æ
6. –ü–∏—à–∏ –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–∞–∫ —é—Ä–∏—Å—Ç: —É—á–∏—Ç—ã–≤–∞–π —Ä–µ–∞–ª—å–Ω—ã–µ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞—Ü–∏–∏.

–¢—Ä–µ–±—É–µ–º—ã–π JSON-—Ñ–æ—Ä–º–∞—Ç (—Å—Ç—Ä–æ–≥–æ):
{{
  "document_type": "(–î–æ–≥–æ–≤–æ—Ä|–ê–∫—Ç|–î–æ–ø.—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ|–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ|–ü–∏—Å—å–º–æ|–î—Ä—É–≥–æ–µ)",
  "brief_description": "(–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ 3-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∫–ª—é—á–µ–≤—ã–µ –¥–µ—Ç–∞–ª–∏, 300-600 —Å–∏–º–≤–æ–ª–æ–≤)",
  "summary": "(–ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ)",
  "responsible_person": "(–§–ò–û –∏–ª–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è, –µ—Å–ª–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã)",
  "deadline": "(–¥–∞—Ç–∞ –∏–ª–∏ —Å—Ä–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã)",
  "amount": "(—Å—É–º–º–∞ –∏ –≤–∞–ª—é—Ç–∞, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã)"
}}

user
–í–æ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:
<<<
{text_input}
>>>
"""

        try:
            response = self.llm_client(
                prompt,
                max_tokens=800,
                temperature=0.01,
                top_p=0.9,
                echo=False
            )

            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –æ—Ç–≤–µ—Ç–∞ –æ—Ç llama_cpp/–æ–±—ë—Ä—Ç–æ–∫
            if isinstance(response, dict) and 'choices' in response:
                result_text = response['choices'][0].get('text', '').strip()
            else:
                result_text = str(response).strip()

            logger.info(f"[DEBUG] Raw LLM output (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤): {result_text[:500]}")
            
            # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π - —ç—Ç–æ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ
            if len(result_text) < 50:
                logger.warning(f"[WARN] LLM output –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π ({len(result_text)} —Å–∏–º–≤): {result_text}")

            # –ü–æ–ø—ã—Ç–∫–∞ –≤—ã—Ç–∞—â–∏—Ç—å JSON –∏–∑ –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–∏
            start = result_text.find('{')
            end = result_text.rfind('}')
            if start != -1 and end != -1 and end > start:
                full_json_str = result_text[start:end+1]
            else:
                # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ —Ç–æ–ª—å–∫–æ —Ç–µ–ª–æ –±–µ–∑ —Ñ–∏–≥—É—Ä–Ω—ã—Ö —Å–∫–æ–±–æ–∫, –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–¥—Å—Ç–∞–≤–∏—Ç—å
                full_json_str = result_text
                if not full_json_str.strip().startswith('{'):
                    full_json_str = '{' + full_json_str
                if not full_json_str.strip().endswith('}'):
                    full_json_str = full_json_str + '}'

            full_json_str = full_json_str.replace('```json', '').replace('```', '').strip()

            # –ü–æ–ø—ã—Ç–∫–∞ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON; –ø—Ä–∏ –æ—à–∏–±–∫–µ ‚Äî —Ä–æ–±—É—Å—Ç–Ω—ã–π —Ñ–æ–ª–ª–±—ç–∫
            try:
                data = json.loads(full_json_str)
            except Exception as e:
                logger.warning(f"JSON parse failed: {e}. –ü–æ–ø—ã—Ç–∫–∞ –≤—ã—Ç—è–Ω—É—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–∏.")
                
                data = {}
                
                # –î–æ—Å—Ç–∞—ë–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ raw LLM output –∏—Å–ø–æ–ª—å–∑—É—è regex –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—è
                fields_to_extract = {
                    'brief_description': [r'brief_description["\']?\s*[:=]\s*["\']?([^"\']*)["\']?(?:,|$)', 
                                         r'–ö—Ä–∞—Ç–∫(?:–æ–µ)?.*?–æ–ø–∏—Å–∞[–Ω–∏]*–µ\s*[:\-]\s*([^,\n]*)'],
                    'responsible_person': [r'responsible_person["\']?\s*[:=]\s*["\']?([^"\']*)["\']?(?:,|$)',
                                          r'–û—Ç–≤–µ—Å—Ç–≤–µ–Ω–Ω(?:—ã–π|—ã–µ)?\s*[:\-]\s*([^,\n]*)'],
                    'deadline': [r'deadline["\']?\s*[:=]\s*["\']?([^"\']*)["\']?(?:,|$)',
                                r'–°—Ä–æ–∫\s*[:\-]\s*([^,\n]*)'],
                    'amount': [r'amount["\']?\s*[:=]\s*["\']?([^"\']*)["\']?(?:,|$)',
                              r'–°—É–º–º–∞\s*[:\-]\s*([^,\n]*)'],
                    'document_type': [r'document_type["\']?\s*[:=]\s*["\']?([^",\n}]*)', 
                                     r'–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞\s*[:\-]\s*([^,\n]*)']
                }
                
                for field, patterns in fields_to_extract.items():
                    for pattern in patterns:
                        m = re.search(pattern, result_text, re.IGNORECASE | re.DOTALL)
                        if m:
                            val = m.group(1).strip()
                            # –û—á–∏—â–∞–µ–º –æ—Ç –∫–∞–≤—ã—á–µ–∫ –∏ –∑–∞–ø—è—Ç—ã—Ö –≤ –∫–æ–Ω—Ü–µ
                            val = re.sub(r'["\',}\n]*$', '', val).strip()
                            if val and val.lower() not in ['none', 'null', 'string']:
                                data[field] = val
                                break
                
                # FALLBACK: –ï—Å–ª–∏ brief_description –≤—Å—ë –µ—â—ë –Ω–µ –Ω–∞–π–¥–µ–Ω, –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                if 'brief_description' not in data or not data.get('brief_description'):
                    # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –ª–∏–Ω–∏–∏
                    cleaned = re.sub(r'[-]{3,}.*?[-]{3,}', '', result_text, flags=re.IGNORECASE)
                    cleaned = re.sub(r'(–¢–ï–ú–ê|user|system|assistant|JSON)[\s:]*', '', cleaned, flags=re.IGNORECASE)
                    # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ 3-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
                    sentences = [s.strip() for s in re.split(r'[.!?]+', cleaned) if s.strip() and len(s.strip()) > 15]
                    if sentences:
                        brief = '. '.join(sentences[:5]).strip() + '.'
                        if len(brief) > 50:
                            data['brief_description'] = brief

            def clean_val(key, default=''):
                val = str(data.get(key, default) or '').strip()
                if val.lower() in ['string', '...', 'null', '', 'none', '—Ç–∏–ø']:
                    return default
                return val

            # –ï—Å–ª–∏ brief_description –ø—É—Å—Ç–æ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º summary –∏–ª–∏ —á–∞—Å—Ç—å –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            brief = clean_val('brief_description', '')
            if not brief:
                brief = clean_val('summary', '')
            if not brief:
                cleaned_text2 = re.sub(r"---[^\n]*---\s*", "", text_input, flags=re.IGNORECASE)
                brief = cleaned_text2.strip()[:400]

            return {
                'document_type': self._normalize_doc_type(clean_val('document_type', '–î–æ–≥–æ–≤–æ—Ä')),
                'brief_description': brief[:1200],
                'description': clean_val('summary', '')[:2000],
                'responsible_person': clean_val('responsible_person', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
                'deadline': clean_val('deadline', '–ù–µ —É–∫–∞–∑–∞–Ω'),
                'amount': clean_val('amount', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')
            }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ LLM –∏–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return self._extract_simple(text_input, email_subject)

    # --- –ü—Ä–æ—Å—Ç—ã–µ –º–µ—Ç–æ–¥—ã –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π ---
    def _extract_simple(self, text: str, subject: str) -> Dict:
        return {
            'document_type': self._detect_document_type(text),
            'description': subject,
            'responsible_person': '–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å',
            'deadline': self._find_date(text),
            'amount': self._find_amount(text)
        }
        
    def _detect_document_type(self, text: str) -> str:
        text = text.lower()
        if '–¥–æ–≥–æ–≤–æ—Ä' in text: return '–î–æ–≥–æ–≤–æ—Ä'
        if '–∞–∫—Ç' in text: return '–ê–∫—Ç'
        return '–î–æ–∫—É–º–µ–Ω—Ç'

    def _find_date(self, text: str) -> str:
        match = re.search(r'\d{2}[./]\d{2}[./]\d{4}', text)
        return match.group(0) if match else '–ù–µ —É–∫–∞–∑–∞–Ω'

    def _find_amount(self, text: str) -> str:
        match = re.search(r'(\d[\d\s,]*)\s*(—Ä—É–±|—Ç–µ–Ω–≥–µ)', text)
        return match.group(0) if match else '–ù–µ —É–∫–∞–∑–∞–Ω–∞'

    def _normalize_doc_type(self, val): return str(val).strip()
    def _clean_person_name(self, val): return str(val).strip()
    def _normalize_date(self, val): return str(val).strip()
    def _normalize_amount(self, val): return str(val).strip()
    
    # ======================== RAG –ú–ï–¢–û–î–´ ========================
    
    def index_templates(self, template_files: List[str]) -> bool:
        """
        –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ —à–∞–±–ª–æ–Ω–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Vector Store –¥–ª—è RAG
        
        Args:
            template_files: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º —à–∞–±–ª–æ–Ω–æ–≤
        
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –∏–Ω–∞—á–µ
        """
        if not self.rag_enabled:
            logger.warning("RAG –Ω–µ –≤–∫–ª—é—á–µ–Ω, –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ")
            return False
        
        try:
            logger.info(f"üìö –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ {len(template_files)} —à–∞–±–ª–æ–Ω–æ–≤...")
            all_documents = []
            
            for template_file in template_files:
                if not Path(template_file).exists():
                    logger.warning(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {template_file}")
                    continue
                
                # –ü–∞—Ä—Å–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
                docs = self.docling_parser.documents_to_langchain(
                    template_file,
                    source_name=Path(template_file).stem
                )
                
                all_documents.extend(docs)
            
            if not all_documents:
                logger.warning("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏—è")
                return False
            
            # –ß–∞–Ω–∫–∏—Ä—É–µ–º
            logger.info(f"  –ß–∞–Ω–∫–∏—Ä–æ–≤–∞–Ω–∏–µ {len(all_documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
            chunks = self.chunking_pipeline.process_pipeline(
                all_documents,
                merge_small=True,
                add_context=True
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ Vector Store
            logger.info(f"  –î–æ–±–∞–≤–ª–µ–Ω–∏–µ {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ Vector Store...")
            success = self.vector_store.add_documents(chunks, batch_size=32)
            
            if success:
                logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(chunks)} —á–∞–Ω–∫–æ–≤")
                return True
            else:
                logger.error("‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ Vector Store")
                return False
        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return False
    
    def extract_info_with_rag(self, email_text: str, email_subject: str, 
                             attachments: list = None) -> Dict:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã –∏–∑ –±–∞–∑—ã 585+ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∫–∞–∫ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è LLM
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –Ω–∞–π—Ç–∏ 3-5 –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        
        Args:
            email_text: –¢–µ–∫—Å—Ç –ø–∏—Å—å–º–∞
            email_subject: –¢–µ–º–∞ –ø–∏—Å—å–º–∞
            attachments: –í–ª–æ–∂–µ–Ω–∏—è –ø–∏—Å—å–º–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        """
        if not self.rag_enabled:
            logger.warning("RAG –Ω–µ –≤–∫–ª—é—á–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑")
            return self.extract_info(email_text, email_subject, attachments)
        
        try:
            logger.info("[RAG] –ê–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ 585+ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤...")
            
            # –®–∞–≥ 1: –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            contract_text = self.extract_text_from_attachments(attachments)
            
            if not contract_text.strip():
                contract_text = email_text
            
            # –û–±—Ä–µ–∑–∞–µ–º –¥–ª—è LLM (Head + Tail –º–µ—Ç–æ–¥)
            if len(contract_text) > 12000:
                full_text = contract_text[:8000] + "\n\n[...—Ç–µ–∫—Å—Ç —Å–æ–∫—Ä–∞—â–µ–Ω –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏...]\n\n" + contract_text[-4000:]
            else:
                full_text = contract_text
            
            # –®–∞–≥ 2: Pre-Retrieval –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ (—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞)
            query = f"–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: {email_subject}. –¢–µ–∫—Å—Ç: {contract_text[:500]}"
            processed_query = self.pre_retrieval.process_query(
                query,
                method="expansion"
            )
            
            logger.info(f"  Pre-Retrieval: —Å–æ–∑–¥–∞–Ω–æ {len(processed_query['variants'])} –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞")
            
            # –®–∞–≥ 3: –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ –≤ –±–∞–∑–µ (–Ω–∞–π—Ç–∏ 5 –ª—É—á—à–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤)
            search_queries = self.pre_retrieval.get_search_queries(processed_query)
            search_results = self.vector_store.search_multiple(search_queries, top_k=5)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            all_results = []
            for results in search_results.values():
                all_results.extend([doc for doc, _ in results])
            
            logger.info(f"  –ü–æ–∏—Å–∫: –Ω–∞–π–¥–µ–Ω–æ {len(all_results)} –ø–æ—Ö–æ–∂–∏—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ –∏–∑ –±–∞–∑—ã")
            
            # –®–∞–≥ 4: Post-Retrieval –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ, —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ)
            if all_results:
                final_docs = self.post_retrieval.process(
                    all_results,
                    query=query,
                    top_k=3,  # –ë–µ—Ä–µ–º —Ç–æ–ø-3 –ª—É—á—à–∏–µ –ø—Ä–∏–º–µ—Ä—ã
                    strategies=["rerank", "summary"]
                )
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤
                context_parts = []
                for i, doc in enumerate(final_docs[:3], 1):
                    context_parts.append(f"\n[–ü–†–ò–ú–ï–† –ö–û–ù–¢–†–ê–ö–¢–ê {i}]\n{doc[:500]}...")
                
                context = "\n".join(context_parts)
            else:
                context = ""
            
            logger.info(f"  Post-Retrieval: –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç ({len(context)} —Å–∏–º–≤–æ–ª–æ–≤)")
            
            # –®–∞–≥ 5: LLM –∞–Ω–∞–ª–∏–∑ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ 585 –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤
            if context:
                enhanced_prompt = f"""system
–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏—Å—Ç —Å –æ–ø—ã—Ç–æ–º –∞–Ω–∞–ª–∏–∑–∞ 585+ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∏—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤.

–ò—Å–ø–æ–ª—å–∑—É–π —Å–ª–µ–¥—É—é—â–∏–µ –ü–†–ò–ú–ï–†–´ –ü–û–•–û–ñ–ò–• –ö–û–ù–¢–†–ê–ö–¢–û–í –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å—Ç–∏–ª—è –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã:
{context}

–¢–µ–ø–µ—Ä—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –Ω–æ–≤—ã–π –∫–æ–Ω—Ç—Ä–∞–∫—Ç. –ò—Å–ø–æ–ª—å–∑—É–π –∑–Ω–∞–Ω–∏—è –∏–∑ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.

user
–ê–ù–ê–õ–ò–ó–ò–†–£–ï–ú–´–ô –ö–û–ù–¢–†–ê–ö–¢:
{full_text}

–¢—Ä–µ–±—É–µ–º—ã–π JSON-—Ñ–æ—Ä–º–∞—Ç:
{{
  "document_type": "(–î–æ–≥–æ–≤–æ—Ä|–ê–∫—Ç|–î–æ–ø.—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ|–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ|–ü–∏—Å—å–º–æ|–î—Ä—É–≥–æ–µ)",
  "brief_description": "(–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ 3-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å –∫–ª—é—á–µ–≤—ã–º–∏ –¥–µ—Ç–∞–ª—è–º–∏, 300-600 —Å–∏–º–≤–æ–ª–æ–≤)",
  "summary": "(–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ)",
  "responsible_person": "(–§–ò–û –∏–ª–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è)",
  "deadline": "(–¥–∞—Ç–∞ –∏–ª–∏ —Å—Ä–æ–∫)",
  "amount": "(—Å—É–º–º–∞ –∏ –≤–∞–ª—é—Ç–∞)"
}}

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û JSON, –±–µ–∑ –¥–æ–ø. —Ç–µ–∫—Å—Ç–∞."""
            else:
                enhanced_prompt = f"""system
–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏—Å—Ç —Å –æ–ø—ã—Ç–æ–º –∞–Ω–∞–ª–∏–∑–∞ 585+ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∏—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤.

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–Ω—Ç—Ä–∞–∫—Ç –∫–∞–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —é—Ä–∏—Å—Ç.

user
–ê–ù–ê–õ–ò–ó–ò–†–£–ï–ú–´–ô –ö–û–ù–¢–†–ê–ö–¢:
{full_text}

–¢—Ä–µ–±—É–µ–º—ã–π JSON-—Ñ–æ—Ä–º–∞—Ç:
{{
  "document_type": "(–î–æ–≥–æ–≤–æ—Ä|–ê–∫—Ç|–î–æ–ø.—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ|–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ|–ü–∏—Å—å–º–æ|–î—Ä—É–≥–æ–µ)",
  "brief_description": "(–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ 3-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, 300-600 —Å–∏–º–≤–æ–ª–æ–≤)",
  "summary": "(–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ)",
  "responsible_person": "(–§–ò–û –∏–ª–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è)",
  "deadline": "(–¥–∞—Ç–∞ –∏–ª–∏ —Å—Ä–æ–∫)",
  "amount": "(—Å—É–º–º–∞ –∏ –≤–∞–ª—é—Ç–∞)"
}}

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û JSON."""
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ LLM
            if self.llm_client:
                response = self.llm_client(
                    enhanced_prompt,
                    max_tokens=800,
                    temperature=0.01,
                    top_p=0.9,
                    echo=False
                )
                
                if isinstance(response, dict):
                    result_text = response['choices'][0].get('text', '').strip()
                else:
                    result_text = str(response).strip()
                
                logger.info(f"[RAG-LLM] –†–µ–∑—É–ª—å—Ç–∞—Ç (–ø–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤): {result_text[:300]}")
                
                # –ü–∞—Ä—Å–∏–º JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                return self._parse_llm_response(result_text, email_subject)
            else:
                logger.warning("LLM client –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑")
                return self._extract_simple(full_text, email_subject)
        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ RAG –∞–Ω–∞–ª–∏–∑–∞: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return self.extract_info(email_text, email_subject, attachments)
    
    def _get_extraction_prompt(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        return """–¢—Ä–µ–±—É–µ–º—ã–π JSON-—Ñ–æ—Ä–º–∞—Ç (—Å—Ç—Ä–æ–≥–æ):
{
  "document_type": "(–î–æ–≥–æ–≤–æ—Ä|–ê–∫—Ç|–î–æ–ø.—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ|–î—Ä—É–≥–æ–µ)",
  "brief_description": "(–†–∞–∑–≤—ë—Ä–Ω—É—Ç–æ–µ —Ä–µ–∑—é–º–µ 3-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –Ω–µ –±–æ–ª–µ–µ 600 —Å–∏–º–≤–æ–ª–æ–≤)",
  "summary": "(–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ)",
  "responsible_person": "(–§–ò–û –∏–ª–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è)",
  "deadline": "(–¥–∞—Ç–∞ –∏–ª–∏ —Å—Ä–æ–∫)",
  "amount": "(—Å—É–º–º–∞ –∏ –≤–∞–ª—é—Ç–∞)"
}"""
    
    def _parse_llm_response(self, result_text: str, email_subject: str) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ LLM"""
        try:
            # –ò—â–µ–º JSON –≤ —Ç–µ–∫—Å—Ç–µ
            start = result_text.find('{')
            end = result_text.rfind('}')
            
            if start != -1 and end != -1:
                json_str = result_text[start:end+1]
                json_str = json_str.replace('```json', '').replace('```', '').strip()
                data = json.loads(json_str)
            else:
                data = {}
        except:
            data = {}
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–µ –∂–µ –ø—Ä–∞–≤–∏–ª–∞ –æ—á–∏—Å—Ç–∫–∏ —á—Ç–æ –∏ —Ä–∞–Ω—å—à–µ
        brief = str(data.get('brief_description', '')).strip()
        if not brief:
            brief = str(data.get('summary', '')).strip()
        
        return {
            'document_type': self._normalize_doc_type(data.get('document_type', '–î–æ–≥–æ–≤–æ—Ä')),
            'brief_description': brief[:1200] if brief else '',
            'description': str(data.get('summary', '')).strip()[:2000],
            'responsible_person': str(data.get('responsible_person', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')).strip(),
            'deadline': str(data.get('deadline', '–ù–µ —É–∫–∞–∑–∞–Ω')).strip(),
            'amount': str(data.get('amount', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')).strip()
        }
