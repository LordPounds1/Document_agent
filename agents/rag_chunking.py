"""RAG Chunking Pipeline - —É–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º"""

import logging
import re
from typing import List, Dict, Optional, Tuple
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter

logger = logging.getLogger(__name__)


class RAGChunkingPipeline:
    """–£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è RAG —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏"""
    
    def __init__(self, 
                 chunk_size: int = 1024,
                 chunk_overlap: int = 256,
                 strategy: str = "recursive"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è pipeline
        
        Args:
            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö (default: 1024)
            chunk_overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ (default: 256)
            strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–∞–∑–±–∏–µ–Ω–∏—è - "recursive" (—É–º–Ω–æ–µ) –∏–ª–∏ "simple" (–ø—Ä–æ—Å—Ç–æ–µ)
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.strategy = strategy
        
        self._init_splitters()
    
    def _init_splitters(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è text splitters"""
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π splitter –¥–ª—è —É–º–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è
        self.recursive_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=[
                "\n\n",           # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
                "\n",             # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å —Å—Ç—Ä–æ–∫
                ". ",             # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
                " ",              # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å —Å–ª–æ–≤
                ""                # Fallback: –ø–æ —Å–∏–º–≤–æ–ª–∞–º
            ],
            length_function=len,
        )
        
        # –ü—Ä–æ—Å—Ç–æ–π character splitter
        self.simple_splitter = CharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separator="\n",
            length_function=len,
        )
        
        logger.info(f"Chunking pipeline –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: size={self.chunk_size}, overlap={self.chunk_overlap}, strategy={self.strategy}")
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏
        
        Args:
            documents: –°–ø–∏—Å–æ–∫ LangChain Documents
        
        Returns:
            –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤-—á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if self.strategy == "recursive":
            return self._split_recursive(documents)
        elif self.strategy == "semantic":
            return self._split_semantic(documents)
        else:
            return self._split_simple(documents)
    
    def _split_recursive(self, documents: List[Document]) -> List[Document]:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ - —Å–∞–º–æ–µ —É–º–Ω–æ–µ"""
        chunks = []
        
        for doc in documents:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π splitter
            texts = self.recursive_splitter.split_text(doc.page_content)
            
            for i, text in enumerate(texts):
                chunk_doc = Document(
                    page_content=text,
                    metadata={
                        **doc.metadata,
                        'chunk_id': i,
                        'chunk_count': len(texts),
                        'strategy': 'recursive'
                    }
                )
                chunks.append(chunk_doc)
        
        logger.info(f"Recursive split: {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ‚Üí {len(chunks)} —á–∞–Ω–∫–æ–≤")
        return chunks
    
    def _split_semantic(self, documents: List[Document]) -> List[Document]:
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ - —Ä–∞–∑–±–∏–≤–∞–µ—Ç –ø–æ —Å–º—ã—Å–ª—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º, –∑–∞–≥–æ–ª–æ–≤–∫–∞–º –∏ —Ç.–¥.
        """
        chunks = []
        
        for doc in documents:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞–∑–¥–µ–ª–∞—Ö
            if doc.metadata.get('type') == 'section':
                # –≠—Ç–æ —É–∂–µ —Ä–∞–∑–¥–µ–ª - —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø–æ–¥—á–∞–Ω–∫–∏
                texts = self.recursive_splitter.split_text(doc.page_content)
                
                for i, text in enumerate(texts):
                    chunk_doc = Document(
                        page_content=text,
                        metadata={
                            **doc.metadata,
                            'chunk_id': i,
                            'chunk_count': len(texts),
                            'strategy': 'semantic'
                        }
                    )
                    chunks.append(chunk_doc)
            else:
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
                # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ (—Å—Ç—Ä–æ–∫–∏ –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å #, —Ç–æ—á–∫–∞.–°–ª–æ–≤–æ, –∏ —Ç.–¥.)
                sections = self._extract_semantic_sections(doc.page_content)
                
                chunk_id = 0
                for section_title, section_text in sections:
                    # –†–∞–∑–±–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ä–∞–∑–¥–µ–ª –Ω–∞ —á–∞–Ω–∫–∏
                    texts = self.recursive_splitter.split_text(section_text)
                    
                    for i, text in enumerate(texts):
                        chunk_doc = Document(
                            page_content=text,
                            metadata={
                                **doc.metadata,
                                'semantic_section': section_title,
                                'chunk_id': chunk_id,
                                'strategy': 'semantic'
                            }
                        )
                        chunks.append(chunk_doc)
                        chunk_id += 1
        
        logger.info(f"Semantic split: {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ‚Üí {len(chunks)} —á–∞–Ω–∫–æ–≤")
        return chunks
    
    def _split_simple(self, documents: List[Document]) -> List[Document]:
        """–ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ - –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—é"""
        chunks = []
        
        for doc in documents:
            texts = self.simple_splitter.split_text(doc.page_content)
            
            for i, text in enumerate(texts):
                chunk_doc = Document(
                    page_content=text,
                    metadata={
                        **doc.metadata,
                        'chunk_id': i,
                        'chunk_count': len(texts),
                        'strategy': 'simple'
                    }
                )
                chunks.append(chunk_doc)
        
        logger.info(f"Simple split: {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ‚Üí {len(chunks)} —á–∞–Ω–∫–æ–≤")
        return chunks
    
    def _extract_semantic_sections(self, text: str) -> List[Tuple[str, str]]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        
        Returns:
            –°–ø–∏—Å–æ–∫ (–∑–∞–≥–æ–ª–æ–≤–æ–∫_—Ä–∞–∑–¥–µ–ª–∞, —Ç–µ–∫—Å—Ç_—Ä–∞–∑–¥–µ–ª–∞)
        """
        sections = []
        
        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ (markdown —Å—Ç–∏–ª—å: # –ó–∞–≥–æ–ª–æ–≤–æ–∫)
        heading_pattern = r'^(#+)\s+(.+?)$'
        lines = text.split('\n')
        
        current_section = "–û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç"
        current_content = []
        
        for line in lines:
            heading_match = re.match(heading_pattern, line)
            
            if heading_match:
                # –ù–∞—à–ª–∏ –Ω–æ–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
                if current_content:
                    sections.append((current_section, '\n'.join(current_content)))
                
                current_section = heading_match.group(2)
                current_content = []
            else:
                if line.strip():  # –¢–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                    current_content.append(line)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑–¥–µ–ª
        if current_content:
            sections.append((current_section, '\n'.join(current_content)))
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ä–∞–∑–¥–µ–ª–æ–≤, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫–∞–∫ –æ–¥–∏–Ω —Ä–∞–∑–¥–µ–ª
        if not sections:
            sections = [("–û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç", text)]
        
        return sections
    
    def merge_small_chunks(self, chunks: List[Document], min_size: int = 512) -> List[Document]:
        """
        –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–∞–ª–µ–Ω—å–∫–∏—Ö —á–∞–Ω–∫–æ–≤ —Å —Å–æ—Å–µ–¥–Ω–∏–º–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        
        Args:
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
            min_size: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä (–º–µ–Ω—å—à–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è —Å —Å–æ—Å–µ–¥—è–º–∏)
        
        Returns:
            –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
        """
        if not chunks:
            return chunks
        
        merged = []
        i = 0
        
        while i < len(chunks):
            current_chunk = chunks[i]
            
            # –ï—Å–ª–∏ —á–∞–Ω–∫ –º–∞–ª–µ–Ω—å–∫–∏–π –∏ —ç—Ç–æ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
            if len(current_chunk.page_content) < min_size and i < len(chunks) - 1:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Å–ª–µ–¥—É—é—â–∏–º —á–∞–Ω–∫–æ–º
                next_chunk = chunks[i + 1]
                merged_content = current_chunk.page_content + "\n\n" + next_chunk.page_content
                
                merged_chunk = Document(
                    page_content=merged_content,
                    metadata={
                        **current_chunk.metadata,
                        'merged': True,
                        'original_chunks': 2
                    }
                )
                merged.append(merged_chunk)
                i += 2  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π —á–∞–Ω–∫
            else:
                merged.append(current_chunk)
                i += 1
        
        logger.info(f"Merge: {len(chunks)} ‚Üí {len(merged)} —á–∞–Ω–∫–æ–≤ (min_size={min_size})")
        return merged
    
    def add_context_windows(self, chunks: List[Document], window_size: int = 2) -> List[Document]:
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –æ–∫–æ–Ω - –¥–æ–±–∞–≤–ª—è–µ—Ç —Å–æ—Å–µ–¥–Ω–∏–µ —á–∞–Ω–∫–∏ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å LLM
        
        Args:
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
            window_size: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤ —Å –∫–∞–∂–¥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã
        
        Returns:
            –ß–∞–Ω–∫–∏ —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–æ—Å–µ–¥—è—Ö
        """
        for i, chunk in enumerate(chunks):
            prev_chunks = chunks[max(0, i - window_size):i]
            next_chunks = chunks[i + 1:min(len(chunks), i + window_size + 1)]
            
            chunk.metadata['prev_chunks'] = len(prev_chunks)
            chunk.metadata['next_chunks'] = len(next_chunks)
            
            # –î–æ–±–∞–≤–ª—è–µ–º IDs —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
            chunk.metadata['prev_chunk_ids'] = [c.metadata.get('chunk_id', -1) for c in prev_chunks]
            chunk.metadata['next_chunk_ids'] = [c.metadata.get('chunk_id', -1) for c in next_chunks]
        
        logger.info(f"Added context windows (window_size={window_size})")
        return chunks
    
    def process_pipeline(self, 
                        documents: List[Document],
                        merge_small: bool = True,
                        add_context: bool = True) -> List[Document]:
        """
        –ü–æ–ª–Ω–∞—è pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        
        Args:
            documents: –í—Ö–æ–¥–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            merge_small: –û–±—ä–µ–¥–∏–Ω—è—Ç—å –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏
            add_context: –î–æ–±–∞–≤–ª—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –æ–∫–Ω–∞
        
        Returns:
            –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –∏ –≥–æ—Ç–æ–≤—ã–µ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ RAG —á–∞–Ω–∫–∏
        """
        logger.info(f"üìä –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏ ({len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
        
        # –≠—Ç–∞–ø 1: –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
        chunks = self.split_documents(documents)
        logger.info(f"  ‚úì –≠—Ç–∞–ø 1: –†–∞–∑–±–∏–µ–Ω–∏–µ ‚Üí {len(chunks)} —á–∞–Ω–∫–æ–≤")
        
        # –≠—Ç–∞–ø 2: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–∞–ª–µ–Ω—å–∫–∏—Ö —á–∞–Ω–∫–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if merge_small:
            chunks = self.merge_small_chunks(chunks)
            logger.info(f"  ‚úì –≠—Ç–∞–ø 2: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ ‚Üí {len(chunks)} —á–∞–Ω–∫–æ–≤")
        
        # –≠—Ç–∞–ø 3: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –æ–∫–æ–Ω (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if add_context:
            chunks = self.add_context_windows(chunks)
            logger.info(f"  ‚úì –≠—Ç–∞–ø 3: –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–±–∞–≤–ª–µ–Ω")
        
        logger.info(f"‚úÖ Pipeline –∑–∞–≤–µ—Ä—à–µ–Ω: {len(chunks)} —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
        return chunks
