#!/usr/bin/env python3
import os
import sys
from pathlib import Path

def check_model():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –º–æ–¥–µ–ª–∏"""
    models_dir = Path("./models")
    models_dir.mkdir(exist_ok=True)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏
    model_files = list(models_dir.glob("*.gguf"))
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ –º–æ–¥–µ–ª–∏ (—Ä–∞–∑–º–µ—Ä > 10 MB)
    valid_models = []
    for model_file in model_files:
        file_size_mb = model_file.stat().st_size / (1024 * 1024)
        if file_size_mb < 10:
            print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {model_file.name} (—Ä–∞–∑–º–µ—Ä: {file_size_mb:.2f} MB)")
            # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            try:
                model_file.unlink()
                print(f"   –£–¥–∞–ª–µ–Ω –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π —Ñ–∞–π–ª")
            except:
                pass
        else:
            valid_models.append(model_file)
    
    if valid_models:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω—ã –≤–∞–ª–∏–¥–Ω—ã–µ –º–æ–¥–µ–ª–∏: {[m.name for m in valid_models]}")
        return str(valid_models[0])
    
    print("‚ùå –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø–∞–ø–∫–µ models/")
    print("\nüì• –°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª–∏ –º–æ–∂–Ω–æ:")
    print("1. Saiga Mistral 7B (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è): https://huggingface.co/IlyaGusev/saiga_mistral_7b_gguf")
    print("2. Saiga Llama 3 8B: https://huggingface.co/IlyaGusev/saiga_llama3_8b_gguf")
    print("3. Saiga Mistral 7B (–∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω–∞—è): https://huggingface.co/IlyaGusev/saiga_mistral_7b_gguf/resolve/main/model-q4_K_M.gguf")
    
    choice = input("\n–°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å? (y/n): ")
    if choice.lower() == 'y':
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å –¥–ª—è –Ω–∞—á–∞–ª–∞
        import requests
        url = "https://huggingface.co/IlyaGusev/saiga_mistral_7b_gguf/resolve/main/model-q4_K_M.gguf"
        model_path = models_dir / "saiga_mistral_7b_q4.gguf"
        
        print(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ {url}...")
        print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ú–æ–¥–µ–ª—å –≤–µ—Å–∏—Ç ~4-5 GB, —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è")
        
        response = requests.get(url, stream=True)
        response.raise_for_status()  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞
        
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        
        with open(model_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    if total_size > 0:
                        percent = (downloaded / total_size) * 100
                        if downloaded % (10 * 1024 * 1024) == 0:  # –ö–∞–∂–¥—ã–µ 10 MB
                            print(f"   –°–∫–∞—á–∞–Ω–æ: {downloaded / (1024*1024):.1f} MB ({percent:.1f}%)")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Å–∫–∞—á–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        file_size_mb = model_path.stat().st_size / (1024 * 1024)
        if file_size_mb < 10:
            print(f"‚ùå –û–®–ò–ë–ö–ê: –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π ({file_size_mb:.2f} MB)")
            print("   –í–æ–∑–º–æ–∂–Ω–æ, —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –Ω–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å")
            model_path.unlink()
            return None
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: {model_path} (—Ä–∞–∑–º–µ—Ä: {file_size_mb:.2f} MB)")
        return str(model_path)
    
    return None

def test_llama_cpp():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ llama-cpp"""
    try:
        from llama_cpp import Llama
        print("‚úÖ llama-cpp-python —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        return True
    except ImportError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ llama-cpp: {e}")
        print("\n–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ llama-cpp-python:")
        print("pip install llama-cpp-python")
        return False

def main():
    print("üîß –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º llama-cpp
    if not test_llama_cpp():
        sys.exit(1)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å
    model_path = check_model()
    
    if model_path:
        print(f"\n‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞: {model_path}")
        print("\n–ß—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å, –¥–æ–±–∞–≤—å—Ç–µ –≤ .env:")
        print(f"MODEL_PATH={model_path}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏...")
        try:
            from llama_cpp import Llama
            llm = Llama(
                model_path=model_path,
                n_ctx=2048,
                n_threads=4,
                verbose=False
            )
            print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            
            # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
            print("\nüß™ –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏...")
            response = llm("–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä!", max_tokens=10)
            print(f"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {response['choices'][0]['text']}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            print("\n–í–æ–∑–º–æ–∂–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è:")
            print("1. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é")
            print("2. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å")
            print("3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø–Ω—É—é –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—É—é –ø–∞–º—è—Ç—å")
    
    print("\n" + "=" * 50)

if __name__ == "__main__":
    main()